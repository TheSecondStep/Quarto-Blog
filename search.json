[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "CLAUDE.md",
    "section": "",
    "text": "æœ¬æ–‡ä»¶ä¸º Claude Code (claude.ai/code) åœ¨æ­¤ä»£ç åº“ä¸­å·¥ä½œæ—¶æä¾›æŒ‡å¯¼ã€‚\n\n\nè¿™æ˜¯ä¸€ä¸ªåŸºäº Quarto çš„æŠ€æœ¯åšå®¢ï¼Œæ‰˜ç®¡äº GitHub Pagesã€‚åšå®¢æ–‡ç« ä»¥ Jupyter notebooks (.ipynb) æˆ– Quarto markdown (.qmd) æ–‡ä»¶ç¼–å†™ã€‚å†…å®¹ä¸ºä¸­æ–‡ï¼Œå¯ç”¨äº† Giscus è¯„è®ºåŠŸèƒ½ã€‚\n\n\n\n\n\n# å®‰è£…/åŒæ­¥ Python ä¾èµ–ï¼ˆä½¿ç”¨ uvï¼ŒPython 3.13ï¼‰\nuv sync\n\n# æœ¬åœ°é¢„è§ˆæœåŠ¡ï¼ˆæ”¯æŒå®æ—¶åˆ·æ–°ï¼‰\nquarto preview\n\n# æ¸²æŸ“/æ„å»ºç½‘ç«™\nquarto render\n\n\n\n\næ¨é€åˆ° main åˆ†æ”¯ä¼šè‡ªåŠ¨è§¦å‘ GitHub Actions éƒ¨ç½²åˆ° gh-pages\næ‰‹åŠ¨å‘å¸ƒï¼šquarto publish gh-pages\n\n\n\n\n\n\nposts/ - åšå®¢æ–‡ç« ï¼Œæ¯ç¯‡æ–‡ç« å­˜æ”¾åœ¨ç‹¬ç«‹å­ç›®å½•ä¸­ï¼ŒåŒ…å« index.ipynb æˆ– index.qmd\nposts/_metadata.yml - æ–‡ç« å…±äº«é…ç½®ï¼ˆå½“å‰è®¾ç½®ï¼šfreeze: true å†»ç»“è®¡ç®—è¾“å‡ºï¼Œtitle-block-banner: true å¯ç”¨æ ‡é¢˜æ¨ªå¹…ï¼‰\nindex.qmd - é¦–é¡µï¼Œå±•ç¤ºæ‰€æœ‰æ–‡ç« åˆ—è¡¨ï¼ˆæŒ‰æ—¥æœŸé™åºæ’åˆ—ï¼Œå¯ç”¨åˆ†ç±»ï¼‰\n_quarto.yml - Quarto ä¸»é…ç½®æ–‡ä»¶ï¼ˆä¸»é¢˜ï¼šflatly + brand è‡ªå®šä¹‰ï¼ŒGiscus è¯„è®ºé…ç½®ä¸º TheSecondStep/Quarto-Blogï¼‰\n.github/workflows/quarto_publish.yml - GitHub Actions CI/CD å·¥ä½œæµ\n\n\n\n\næ–‡ç« ï¼šæ¯ç¯‡åšå®¢æ–‡ç« æ˜¯ posts/ ä¸‹çš„å­ç›®å½•ï¼ŒåŒ…å« index.ipynb æˆ– index.qmd æ–‡ä»¶ã€‚é¦–é¡µåˆ—è¡¨ä¼šè‡ªåŠ¨å‘ç°å¹¶å±•ç¤ºè¿™äº›æ–‡ç« ã€‚\nFreeze æ¨¡å¼ï¼šè®¡ç®—è¾“å‡ºè¢«å†»ç»“ï¼ˆposts/_metadata.yml ä¸­è®¾ç½® freeze: trueï¼‰ä»¥ç¡®ä¿æ„å»ºå¯å¤ç°ã€‚ç¼–è¾‘åŒ…å«ä»£ç çš„ notebook æ—¶ï¼Œå¯èƒ½éœ€è¦ä¸´æ—¶ç¦ç”¨ freeze æˆ–æ‰‹åŠ¨é‡æ–°æ¸²æŸ“å•å…ƒæ ¼ã€‚\nåˆ†ç±»ï¼šé€šè¿‡ index.qmd ä¸­çš„ categories: true åœ¨åˆ—è¡¨é¡µå¯ç”¨åˆ†ç±»æ˜¾ç¤ºã€‚\n\n\n\n\nåŸºç¡€ä¸»é¢˜ï¼šflatly (Bootswatch)\nè‡ªå®šä¹‰ä¸»é¢˜å±‚ï¼šbrand\nè‡ªå®šä¹‰ CSSï¼šstyles.css\nä»£ç å—ï¼šå¯ç”¨å¤åˆ¶åŠŸèƒ½ã€å¯ç”¨è¾¹æ¡†"
  },
  {
    "objectID": "CLAUDE.html#é¡¹ç›®æ¦‚è¿°",
    "href": "CLAUDE.html#é¡¹ç›®æ¦‚è¿°",
    "title": "CLAUDE.md",
    "section": "",
    "text": "è¿™æ˜¯ä¸€ä¸ªåŸºäº Quarto çš„æŠ€æœ¯åšå®¢ï¼Œæ‰˜ç®¡äº GitHub Pagesã€‚åšå®¢æ–‡ç« ä»¥ Jupyter notebooks (.ipynb) æˆ– Quarto markdown (.qmd) æ–‡ä»¶ç¼–å†™ã€‚å†…å®¹ä¸ºä¸­æ–‡ï¼Œå¯ç”¨äº† Giscus è¯„è®ºåŠŸèƒ½ã€‚"
  },
  {
    "objectID": "CLAUDE.html#å¸¸ç”¨å‘½ä»¤",
    "href": "CLAUDE.html#å¸¸ç”¨å‘½ä»¤",
    "title": "CLAUDE.md",
    "section": "",
    "text": "# å®‰è£…/åŒæ­¥ Python ä¾èµ–ï¼ˆä½¿ç”¨ uvï¼ŒPython 3.13ï¼‰\nuv sync\n\n# æœ¬åœ°é¢„è§ˆæœåŠ¡ï¼ˆæ”¯æŒå®æ—¶åˆ·æ–°ï¼‰\nquarto preview\n\n# æ¸²æŸ“/æ„å»ºç½‘ç«™\nquarto render\n\n\n\n\næ¨é€åˆ° main åˆ†æ”¯ä¼šè‡ªåŠ¨è§¦å‘ GitHub Actions éƒ¨ç½²åˆ° gh-pages\næ‰‹åŠ¨å‘å¸ƒï¼šquarto publish gh-pages"
  },
  {
    "objectID": "CLAUDE.html#é¡¹ç›®ç»“æ„",
    "href": "CLAUDE.html#é¡¹ç›®ç»“æ„",
    "title": "CLAUDE.md",
    "section": "",
    "text": "posts/ - åšå®¢æ–‡ç« ï¼Œæ¯ç¯‡æ–‡ç« å­˜æ”¾åœ¨ç‹¬ç«‹å­ç›®å½•ä¸­ï¼ŒåŒ…å« index.ipynb æˆ– index.qmd\nposts/_metadata.yml - æ–‡ç« å…±äº«é…ç½®ï¼ˆå½“å‰è®¾ç½®ï¼šfreeze: true å†»ç»“è®¡ç®—è¾“å‡ºï¼Œtitle-block-banner: true å¯ç”¨æ ‡é¢˜æ¨ªå¹…ï¼‰\nindex.qmd - é¦–é¡µï¼Œå±•ç¤ºæ‰€æœ‰æ–‡ç« åˆ—è¡¨ï¼ˆæŒ‰æ—¥æœŸé™åºæ’åˆ—ï¼Œå¯ç”¨åˆ†ç±»ï¼‰\n_quarto.yml - Quarto ä¸»é…ç½®æ–‡ä»¶ï¼ˆä¸»é¢˜ï¼šflatly + brand è‡ªå®šä¹‰ï¼ŒGiscus è¯„è®ºé…ç½®ä¸º TheSecondStep/Quarto-Blogï¼‰\n.github/workflows/quarto_publish.yml - GitHub Actions CI/CD å·¥ä½œæµ"
  },
  {
    "objectID": "CLAUDE.html#å†…å®¹æ¶æ„",
    "href": "CLAUDE.html#å†…å®¹æ¶æ„",
    "title": "CLAUDE.md",
    "section": "",
    "text": "æ–‡ç« ï¼šæ¯ç¯‡åšå®¢æ–‡ç« æ˜¯ posts/ ä¸‹çš„å­ç›®å½•ï¼ŒåŒ…å« index.ipynb æˆ– index.qmd æ–‡ä»¶ã€‚é¦–é¡µåˆ—è¡¨ä¼šè‡ªåŠ¨å‘ç°å¹¶å±•ç¤ºè¿™äº›æ–‡ç« ã€‚\nFreeze æ¨¡å¼ï¼šè®¡ç®—è¾“å‡ºè¢«å†»ç»“ï¼ˆposts/_metadata.yml ä¸­è®¾ç½® freeze: trueï¼‰ä»¥ç¡®ä¿æ„å»ºå¯å¤ç°ã€‚ç¼–è¾‘åŒ…å«ä»£ç çš„ notebook æ—¶ï¼Œå¯èƒ½éœ€è¦ä¸´æ—¶ç¦ç”¨ freeze æˆ–æ‰‹åŠ¨é‡æ–°æ¸²æŸ“å•å…ƒæ ¼ã€‚\nåˆ†ç±»ï¼šé€šè¿‡ index.qmd ä¸­çš„ categories: true åœ¨åˆ—è¡¨é¡µå¯ç”¨åˆ†ç±»æ˜¾ç¤ºã€‚"
  },
  {
    "objectID": "CLAUDE.html#ä¸»é¢˜ä¸æ ·å¼",
    "href": "CLAUDE.html#ä¸»é¢˜ä¸æ ·å¼",
    "title": "CLAUDE.md",
    "section": "",
    "text": "åŸºç¡€ä¸»é¢˜ï¼šflatly (Bootswatch)\nè‡ªå®šä¹‰ä¸»é¢˜å±‚ï¼šbrand\nè‡ªå®šä¹‰ CSSï¼šstyles.css\nä»£ç å—ï¼šå¯ç”¨å¤åˆ¶åŠŸèƒ½ã€å¯ç”¨è¾¹æ¡†"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html",
    "href": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html",
    "title": "åŸºäºQuartoå’ŒGitHub Pagesçš„ä¸ªäººåšå®¢ç½‘ç«™æ­å»ºæµç¨‹",
    "section": "",
    "text": "åŸºäºGithub Actionsè‡ªåŠ¨æ¸²æŸ“Quartoåšå®¢\næœ¬ç¯‡åšå®¢æ—¨åœ¨æä¾›å®Œæ•´çš„åŸºäºQuartoå’ŒGitHub Pagesçš„ä¸ªäººåšå®¢ç½‘ç«™æ­å»ºæµç¨‹ï¼Œå¸®åŠ©è¯»è€…å¿«é€Ÿæ­å»ºä¸ªäººåšå®¢ç³»ç»Ÿã€‚"
  },
  {
    "objectID": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html#åˆ›å»ºquarto-blogé¡¹ç›®",
    "href": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html#åˆ›å»ºquarto-blogé¡¹ç›®",
    "title": "åŸºäºQuartoå’ŒGitHub Pagesçš„ä¸ªäººåšå®¢ç½‘ç«™æ­å»ºæµç¨‹",
    "section": "åˆ›å»ºQuarto Blogé¡¹ç›®",
    "text": "åˆ›å»ºQuarto Blogé¡¹ç›®\n\næ‰“å¼€cursorå‘½ä»¤é¢æ¿ï¼Œè¾“å…¥Quarto:Create Projectåˆ›å»ºé¡¹ç›®\n\né€‰æ‹©Blog Project\n\né€‰æ‹©ä¸€ä¸ªæ–‡ä»¶å¤¹ä½œä¸ºåšå®¢é¡¹ç›®çš„æ ¹ç›®å½•\n\nä¸ºåšå®¢é¡¹ç›®èµ·å\n\nquartoå°†ä¼šåˆ›å»ºé»˜è®¤é…ç½®ä»¥æ”¯æŒåšå®¢ç½‘ç«™åŠŸèƒ½\n\nç‚¹å‡»å³ä¸Šè§’â€œé¢„è§ˆâ€æŒ‰é’®ï¼Œquartoå°†ä¸ºä½ æ¸²æŸ“åšå®¢ç½‘ç«™\n\n\nè¿™æ˜¯åœ¨å…¥é—¨åšå®¢é¡¹ç›®ä¸­åˆ›å»ºçš„å…³é”®æ–‡ä»¶æ‘˜è¦ï¼š\n\n\n\næ–‡ä»¶\næè¿°\n\n\n\n\n_quarto.yml\nQuarto é¡¹ç›®æ–‡ä»¶\n\n\nindex.qmd\nåšå®¢ä¸»é¡µ\n\n\nabout.qmd\nå…³äºé¡µé¢\n\n\nposts/\nåŒ…å«æ–‡ç« çš„ç›®å½•\n\n\nposts/_metadata.yml\nposts çš„å…±äº«é€‰é¡¹\n\n\nstyles.css\nç½‘ç«™çš„å®šåˆ¶ CSS\n\n\n\nä¸»é¡µé¢„è§ˆç•Œé¢"
  },
  {
    "objectID": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html#é¡¹ç›®æ–‡ä»¶ä»‹ç»",
    "href": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html#é¡¹ç›®æ–‡ä»¶ä»‹ç»",
    "title": "åŸºäºQuartoå’ŒGitHub Pagesçš„ä¸ªäººåšå®¢ç½‘ç«™æ­å»ºæµç¨‹",
    "section": "é¡¹ç›®æ–‡ä»¶ä»‹ç»",
    "text": "é¡¹ç›®æ–‡ä»¶ä»‹ç»\n\nä¸»é¡µ\nä¸»é¡µæ˜¯å…³äºpostsç›®å½•ä¸‹æ‰€æœ‰åšå®¢çš„æ–‡ç« åˆ—è¡¨ï¼Œå¦‚ä¸»é¡µé¢„è§ˆç•Œé¢æ‰€ç¤ºã€‚\næºä»£ç ï¼š\n---\ntitle: \"blog_tutorial\"\nlisting:\n  contents: posts\n  feed: true\n  sort: \"date desc\"\n  type: default\n  categories: true\n  sort-ui: false\n  filter-ui: false\npage-layout: full\ntitle-block-banner: true\n---\n\n\n\nå­—æ®µ\næè¿°\n\n\n\n\ntitle\næ ‡é¢˜\n\n\ncategories\nå¼€å¯åšå®¢åˆ†ç±»\n\n\n\n\nCategoriesï¼ˆåˆ†ç±»ï¼‰\ncategories: trueè¡¨ç¤ºé…ç½®ä¸ºå¯ç”¨åˆ†ç±»ï¼Œè¿™äº›åˆ†ç±»æ˜¾ç¤ºåœ¨ä¸»é¡µçš„å³ä¾§è¾¹ç¼˜ã€‚\nåˆ†ç±»ä»åŒ…å«åœ¨åˆ—è¡¨ä¸­çš„æ–‡æ¡£çš„å‰ç½®å†…å®¹ä¸­è¯»å–ã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªåŒ…å«åˆ†ç±»çš„æ ·æœ¬å¸–å­å…ƒæ•°æ®ï¼š\n---\ntitle: \"Post With Code\"\ndescription: \"Post description\"\nauthor: \"Fizz McPhee\"\ndate: \"5/22/2021\"\ncategories:\n  - news\n  - code\n  - analysis\n---\n\n\nå…³äºé¡µé¢\nabout.qmdåŒ…å«æœ‰å…³åšå®¢åŠå…¶ä½œè€…çš„æ›´å¤šä¿¡æ¯ã€‚\nabout.qmdæºä»£ç å¯èƒ½çš„æ ·å­ï¼š\n---\ntitle: \"About\"\nimage: profile.jpg\nabout:\n  template: jolla\n  links:\n    - icon: twitter\n      text: Twitter\n      href: https://twitter.com\n    - icon: linkedin\n      text: LinkedIn\n      href: https://linkedin.com\n    - icon: github\n      text: Github\n      href: https://github.com\n---\n\nAbout this blog\n\n\npostsæ–‡ä»¶å¤¹\næ„æˆåšå®¢å†…å®¹çš„å¸–å­ä½äºpostsç›®å½•ä¸­ã€‚\né€šè¿‡åœ¨postså†…åˆ›å»ºä¸€ä¸ªå­ç›®å½•ï¼Œå¹¶å‘è¯¥ç›®å½•æ·»åŠ ä¸€ä¸ªindex.ipynbï¼ˆæˆ–æ˜¯ index.qmdï¼‰æ–‡ä»¶æ¥ä¸ºä½ çš„åšå®¢æ·»åŠ ä¸€ç¯‡æ–°å¸–å­ã€‚è¿™ä¸ªæ–‡ä»¶å°±æ˜¯æ–°çš„åšå®¢å¸–å­ï¼Œå½“ä½ æ¸²æŸ“(quarto render)å®ƒæ—¶ï¼Œåšå®¢ä¸»é¡µå°†è‡ªåŠ¨æ›´æ–°ï¼Œåœ¨åˆ—è¡¨é¡¶éƒ¨åŒ…å«æœ€æ–°çš„å¸–å­ã€‚"
  },
  {
    "objectID": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html#å‘å¸ƒpublishing",
    "href": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html#å‘å¸ƒpublishing",
    "title": "åŸºäºQuartoå’ŒGitHub Pagesçš„ä¸ªäººåšå®¢ç½‘ç«™æ­å»ºæµç¨‹",
    "section": "å‘å¸ƒï¼ˆPublishingï¼‰",
    "text": "å‘å¸ƒï¼ˆPublishingï¼‰\næœ‰å¤šç§æ–¹å¼å‘å¸ƒQuartoåšå®¢ã€‚æœ¬æ–‡ä»…ä»‹ç»åŸºäºGitHub Actionçš„GitHub Pageså‘å¸ƒæ–¹å¼ã€‚\n\nåœ¨githubåˆ›å»ºæ–°ä»“åº“\n\n\n\næœ¬åœ°é¡¹ç›®åˆå§‹åŒ–å¹¶å…³è”githubä»“åº“\n\nåˆ›å»ºREADME.mdæ–‡ä»¶ï¼Œå†…å®¹ä¸º# ä½ çš„ä»“åº“å\nåˆ›å»º.gitignoreæ–‡ä»¶\n\n/.quarto/\n/_site/\n\nä¿®æ”¹_quarto.ymlä¸­site-urlä¸ºä½ çš„ä»“åº“åœ°å€\næäº¤ä¿®æ”¹å¹¶å…³è”è¿œç«¯ä»“åº“\n\ngit init\ngit add .\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin git@github.com:TheSecondStep/quarto-blog-tutorial.git\ngit push -u origin main\næœ¬åœ°æ‰§è¡Œä¸€æ¬¡å‘½ä»¤è¡Œquarto publish gh-pages quartoå°†ä¸ºä½ åˆ›å»ºgh-pagesåˆ†æ”¯ï¼Œç”¨äºGitHub Pageséƒ¨ç½²ç½‘ç«™ä½¿ç”¨ã€‚\néƒ¨ç½²GitHub Action GitHub Actionå¯ä»¥æ”¯æŒåœ¨mainåˆ†æ”¯ä¿®æ”¹ä¸Šä¼ ä»£ç åˆ°githubä»“åº“åï¼Œè‡ªåŠ¨è§¦å‘å·¥ä½œæµï¼Œæ¸²æŸ“åšå®¢ç½‘ç«™å¹¶æ›´æ–°ã€‚\nåˆ›å»º.github/workflows/publish.ymlï¼Œå†…å®¹å¦‚ä¸‹\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\nåœ¨githubä»“åº“è®¾ç½®çš„Actionséƒ¨åˆ†çš„Workflow permissionså‹¾é€‰Read and write permissionsã€‚\næ¨é€ä¸Šè¿°ä¿®æ”¹åˆ°githubï¼Œåç»­åªè¦æ‰‹åŠ¨ä¿®æ”¹åšå®¢å¹¶æ¨é€åˆ°mainåˆ†æ”¯ï¼Œå°†è‡ªåŠ¨è§¦å‘GitHub Actionæ¸²æŸ“ç½‘ç«™ã€‚\n\n\n\næ‰§è¡Œä»£ç \nGitHub Actionä¹Ÿæ”¯æŒé…ç½®è‡ªåŠ¨æ‰§è¡Œä»£ç ï¼Œæ¥ä½œä¸ºä½œä¸ºç½‘ç«™æ¸²æŸ“çš„ä¸€éƒ¨åˆ†ã€‚\n\nğŸ’¡ ç¡®ä¿ä½ çš„ä»£ç èƒ½å¤Ÿåœ¨GitHub Actionä¸­æ‰§è¡ŒæˆåŠŸçš„æœ€ä½³å®è·µæ˜¯é€šè¿‡è™šæ‹Ÿç¯å¢ƒç®¡ç†ï¼Œå¦‚venv\næœ¬æ–‡ä»¥uvç®¡ç†venvè™šæ‹Ÿç¯å¢ƒä¸ºä¾‹ï¼Œç¡®ä¿ä½ çš„æœ¬åœ°ç¯å¢ƒå·²ç»é€šè¿‡è™šæ‹Ÿç¯å¢ƒç®¡ç†pythonä»£ç ä¸”åŒæ­¥åˆ°GitHub\n\nä¿®æ”¹publish.ymlï¼Œç„¶åæ¨é€åˆ°è¿œç«¯åˆ†æ”¯\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v5\n        with:\n          enable-cache: true\n          python-version: '3.13' #ä½ çš„è™šæ‹Ÿç¯å¢ƒçš„pythonç‰ˆæœ¬\n\n      - name: Install Dependencies\n        run: |\n          uv sync\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          QUARTO_PYTHON: .venv/bin/python"
  },
  {
    "objectID": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html#æ·»åŠ è¯„è®ºåŒº",
    "href": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html#æ·»åŠ è¯„è®ºåŒº",
    "title": "åŸºäºQuartoå’ŒGitHub Pagesçš„ä¸ªäººåšå®¢ç½‘ç«™æ­å»ºæµç¨‹",
    "section": "æ·»åŠ è¯„è®ºåŒº",
    "text": "æ·»åŠ è¯„è®ºåŒº\n\nè‹¥æ€æƒ³æ²¡æœ‰å›å“ï¼Œæ–‡å­—ä¾¿åªæ˜¯ä¸€åœºæ°¸æ— æ­¢å¢ƒçš„ç‹¬ç™½ï¼›è¯„è®ºåŒºå¹¶éç‚¹ç¼€ï¼Œè€Œæ˜¯åšå®¢å¾—ä»¥å‘¼å¸çš„è‚ºå¶ã€‚\n\n\nç™»å½•https://giscus.app/zh-CN å¹¶æ ¹æ®æŒ‡å¼•æ“ä½œï¼Œ giscusæ˜¯åˆ©ç”¨GitHub Discussionså®ç°çš„è¯„è®ºç³»ç»Ÿï¼Œå¤©ç„¶ä¸æˆ‘ä»¬è¿™å¥—åŸºäºGithubçš„åšå®¢ç³»ç»Ÿé€‚é…ã€‚\nç„¶åæˆ‘ä»¬ä¼šçœ‹åˆ°ç±»ä¼¼è¿™ä¹ˆä¸€æ®µé…ç½®\n\nä¿®æ”¹_quarto.ymlï¼ŒåŠ ä¸Šå¯¹åº”é…ç½®\n\nä¸Šä¼ æ‰€æœ‰ä¿®æ”¹ï¼Œæˆ‘ä»¬çš„ç½‘ç«™å°±æ”¯æŒè¯„è®ºåŠŸèƒ½äº†"
  },
  {
    "objectID": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html#ç»“è¯­",
    "href": "posts/Quartoä¸ªäººåšå®¢æ­å»º/index.html#ç»“è¯­",
    "title": "åŸºäºQuartoå’ŒGitHub Pagesçš„ä¸ªäººåšå®¢ç½‘ç«™æ­å»ºæµç¨‹",
    "section": "ç»“è¯­",
    "text": "ç»“è¯­\nè‡³æ­¤ï¼ŒåŸºäºQuartoçš„ä¸ªäººç½‘ç«™å·²ç»éƒ¨ç½²ä¸Šçº¿ï¼Œæ”¯æŒé€šè¿‡GitHub Pagesè®¿é—®ï¼Œé€šè¿‡GitHub Actionè‡ªåŠ¨åŒ–æ¸²æŸ“ã€‚\nå¦‚æœä½ åœ¨å®é™…æ“ä½œä¸­é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿åœ¨è¯„è®ºåŒºç•™è¨€ã€‚"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Attention is all you need",
    "section": "",
    "text": "åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ\n\n\n\ndeep learning\n\nfastai\n\n\n\n\n\n\n\n\n\nJan 30, 2026\n\n\né€ƒä¹‹å¤­å¤­\n\n\n\n\n\n\n\n\n\n\n\n\nåŸºäºQuartoå’ŒGitHub Pagesçš„ä¸ªäººåšå®¢ç½‘ç«™æ­å»ºæµç¨‹\n\n\n\ntools\n\n\n\n\n\n\n\n\n\nJan 25, 2026\n\n\né€ƒä¹‹å¤­å¤­\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html",
    "href": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html",
    "title": "åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ",
    "section": "",
    "text": "è®­ç»ƒä½ çš„ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œ"
  },
  {
    "objectID": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#ç¥ç»ç½‘ç»œè®­ç»ƒæµç¨‹",
    "href": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#ç¥ç»ç½‘ç»œè®­ç»ƒæµç¨‹",
    "title": "åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ",
    "section": "ç¥ç»ç½‘ç»œè®­ç»ƒæµç¨‹",
    "text": "ç¥ç»ç½‘ç»œè®­ç»ƒæµç¨‹\nä¸‹å›¾æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œåŸºæœ¬çš„ä¸ƒä¸ªæ­¥éª¤ï¼š\n\nåˆå§‹åŒ–æƒé‡å‚æ•°ï¼Œä¸€èˆ¬é‡‡ç”¨éšæœºåŒ–æƒé‡ä¹Ÿä¼šæœ‰è‰¯å¥½çš„æ•ˆæœï¼›\né€šè¿‡ç¥ç»ç½‘ç»œçš„è®¡ç®—ï¼Œå¾—åˆ°é¢„æµ‹å€¼ï¼›\nè®¡ç®—æŸå¤±å‡½æ•°ï¼ŒæŸå¤±å‡½æ•°æ˜¯è¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´å·®å¼‚çš„å‡½æ•°ï¼ŒæŸå¤±å‡½æ•°è¶Šå°ï¼Œé¢„æµ‹å€¼ä¸çœŸå®å€¼è¶Šæ¥è¿‘ï¼›\nè®¡ç®—æ¢¯åº¦ï¼Œé‡‡ç”¨SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰ç®—æ³•è®¡ç®—æ¢¯åº¦ï¼Œæ¢¯åº¦æ˜¯æŸå¤±å‡½æ•°å¯¹æƒé‡å‚æ•°çš„åå¯¼æ•°ï¼Œè¡¨ç¤ºæŸå¤±å‡½æ•°åœ¨æƒé‡å‚æ•°å¤„çš„å˜åŒ–ç‡ï¼›\næ›´æ–°æƒé‡å‚æ•°ï¼Œæ ¹æ®æ¢¯åº¦æ›´æ–°æƒé‡å‚æ•°ï¼Œä½¿å¾—æŸå¤±å‡½æ•°æœ€å°åŒ–ï¼›\né‡å¤æ­¥éª¤2-5ï¼Œç›´åˆ°æŸå¤±å‡½æ•°æ”¶æ•›ï¼›\nåœæ­¢ç¥ç»ç½‘ç»œè®­ç»ƒï¼Œä¸€èˆ¬å–å†³äºä¸¤ç§æƒ…å†µï¼š\n\nè¾¾åˆ°é¢„è®¾çš„è®­ç»ƒæ¬¡æ•°ï¼›\næŸå¤±å‡½æ•°æ”¶æ•›åˆ°é¢„è®¾çš„é˜ˆå€¼ã€‚\n\n\n\n\n\n\n\ngraph LR\n  init --&gt; predict --&gt; loss --&gt; gradient --&gt; step --&gt; stop\n  step -- repeat --&gt; predict"
  },
  {
    "objectID": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#mnistæ•°æ®é›†",
    "href": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#mnistæ•°æ®é›†",
    "title": "åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ",
    "section": "MNISTæ•°æ®é›†",
    "text": "MNISTæ•°æ®é›†\nMNISTæ•°æ®é›†æ˜¯æ‰‹å†™æ•°å­—ï¼ˆ0-9ï¼‰çš„å›¾ç‰‡æ•°æ®é›†ï¼ŒåŒ…å«60000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ10000ä¸ªæµ‹è¯•æ ·æœ¬ã€‚æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ª28x28çš„ç°åº¦å›¾åƒï¼Œè¡¨ç¤ºä¸€ä¸ªæ‰‹å†™æ•°å­—ã€‚\nåŠ è½½MNISTæ•°æ®é›†\n\n# fastaiè¯¾ç¨‹é¢„è®¾ç½®å¯¼å…¥åŒ…\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.MNIST)\n\næŸ¥çœ‹MNISTæ•°æ®é›†ç›®å½•ç»“æ„\n\npath.ls()\n\n[Path('D:/User/ChenBo/.fastai/data/mnist_png/testing'), Path('D:/User/ChenBo/.fastai/data/mnist_png/training')]\n\n\n\ntraining_path = path/'training'\ntraining_path.ls()\n\n[Path('D:/User/ChenBo/.fastai/data/mnist_png/training/0'), Path('D:/User/ChenBo/.fastai/data/mnist_png/training/1'), Path('D:/User/ChenBo/.fastai/data/mnist_png/training/2'), Path('D:/User/ChenBo/.fastai/data/mnist_png/training/3'), Path('D:/User/ChenBo/.fastai/data/mnist_png/training/4'), Path('D:/User/ChenBo/.fastai/data/mnist_png/training/5'), Path('D:/User/ChenBo/.fastai/data/mnist_png/training/6'), Path('D:/User/ChenBo/.fastai/data/mnist_png/training/7'), Path('D:/User/ChenBo/.fastai/data/mnist_png/training/8'), Path('D:/User/ChenBo/.fastai/data/mnist_png/training/9')]\n\n\n\ntesting_path = path/'testing'\ntesting_path.ls()\n\n[Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/0'), Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/1'), Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/2'), Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/3'), Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/4'), Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/5'), Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/6'), Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/7'), Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/8'), Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/9')]\n\n\næ•°æ®é›†ä¸­çš„æ¯ä¸€å¼ å›¾ç‰‡éƒ½æ˜¯ç±»ä¼¼ä¸‹å›¾æ‰€ç¤ºçš„ä¸€å¼ æ‰‹å†™é˜¿æ‹‰ä¼¯æ•°å­—\n\nzero_path = training_path/'0'\nimg_0 = Image.open(zero_path.ls()[0])\nimg_0"
  },
  {
    "objectID": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#å¼€å§‹è®­ç»ƒ",
    "href": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#å¼€å§‹è®­ç»ƒ",
    "title": "åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ",
    "section": "å¼€å§‹è®­ç»ƒ",
    "text": "å¼€å§‹è®­ç»ƒ\n\næ„å»ºç¥ç»ç½‘ç»œè¾“å…¥\nè®¡ç®—æœºå¹¶ä¸è®¤è¯†å›¾ç‰‡ï¼Œéœ€è¦å°†å›¾ç‰‡è½¬æ¢ä¸ºæ•°å­—ï¼Œæ‰èƒ½è¿›è¡Œè®­ç»ƒã€‚\næ¯å¼ å›¾ç‰‡ç”±28 * 28çš„åƒç´ ç‚¹ç»„æˆï¼Œæ¯ä¸ªåƒç´ ç‚¹ç”±0-255çš„ç°åº¦å€¼è¡¨ç¤ºï¼Œ0è¡¨ç¤ºç™½è‰²ï¼Œ255è¡¨ç¤ºé»‘è‰²ã€‚ï¼ˆç°ä»£çš„å½©è‰²å›¾ç‰‡æ˜¯åŸºäºRGBä¸‰ä¸ªé€šé“ç¡®å®šä¸€ä¸ªåƒç´ ç‚¹ï¼‰\nç¥ç»ç½‘ç»œéœ€è¦åŸºäºçŸ©é˜µä¹˜æ³•è¿›è¡Œå¤§é‡å¹¶è¡Œè®¡ç®—ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†[28 * 28]åƒç´ ç‚¹å‹ç¼©åˆ°ä¸€ä¸ªç»´åº¦ä¸Šï¼Œæ„æˆä¸€ä¸ª784ç»´çš„å‘é‡ï¼ˆä¸€é˜¶å¼ é‡ï¼‰ã€‚\næœ€ç»ˆ60000å¼ è®­ç»ƒé›†å›¾ç‰‡æ„æˆ[60000, 784]çš„çŸ©é˜µï¼ˆäºŒé˜¶å¼ é‡ï¼‰\n\n\nåˆ›å»ºåŸºåº§æ¨¡å‹\nä¸ºäº†ä½¿åç»­çš„æ¨¡å‹è®­ç»ƒæœ‰å‚ç…§ï¼Œæ¯ä¸€ä¸ªç¥ç»ç½‘ç»œè®­ç»ƒä¹‹å‰éƒ½åº”è¯¥å®šä¹‰ä¸€ä¸ªå¯ä»¥å¿«é€Ÿå®ç°çš„åŸºåº§æ¨¡å‹ã€‚\nä¸€ä¸ªç®€å•æ˜äº†çš„æ€è·¯æ˜¯åƒç´ ç›¸ä¼¼åº¦ã€‚åŸºäºè®­ç»ƒé›†å›¾åƒè®¡ç®—å‡ºæ¯ä¸ªæ•°å­—ï¼ˆ0-9ï¼‰çš„æ‰€æœ‰å›¾åƒçš„åƒç´ å¹³å‡å€¼ï¼Œåˆ†ç±»æ—¶è®¡ç®—æ¨¡å‹å’ŒéªŒè¯é›†çš„L1èŒƒå¼ï¼ˆä¹Ÿæˆä¸ºå¹³å‡ç»å¯¹å·®ï¼‰å€¼ï¼Œåˆ†ç±»ä¸ºå·®å€¼æœ€å°çš„æ•°å­—\nç”±äºè¯¥ç®€å•æ¨¡å‹ç¡®å®šä¸ºåƒç´ å¹³å‡å€¼ï¼Œå› æ­¤ä¸éœ€è¦è¿›è¡Œè®­ç»ƒï¼Œæ­¥éª¤ç®€åŒ–ä¸ºï¼š\n\n\n\n\n\ngraph LR\n  predict --&gt; loss --&gt; stop\n\n\n\n\n\n\n\n# å°†è®­ç»ƒé›†0-9çš„PathåŠ è½½åˆ°ä¸€ä¸ªlistä¸­\ntraining_paths = [training_path/f'{i}' for i in range(10)]\ntraining_paths\n\n[Path('D:/User/ChenBo/.fastai/data/mnist_png/training/0'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/training/1'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/training/2'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/training/3'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/training/4'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/training/5'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/training/6'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/training/7'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/training/8'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/training/9')]\n\n\n\n# å°†è®­ç»ƒé›†æ‰€æœ‰å›¾åƒè½¬æ¢ä¸ºtensorï¼Œæ„å»ºä¸ºä¸€ä¸ªåŒ…å«60000ä¸ªtensorçš„listï¼Œæ¯ä¸ªtensorä¸º[28, 28]çš„äºŒé˜¶å¼ é‡\ntraining_nums = [[tensor(Image.open(image)) for image in path.ls()] for path in training_paths]\nlen(training_nums)\n\n10\n\n\n\nâ“ ä¸ºä»€ä¹ˆä¸€ä¸ªImageå¯¹è±¡å¯ä»¥é€šè¿‡tensoræ„é€ å‡½æ•°è½¬æ¢ä¸ºtensor\næ ¸å¿ƒåœ¨äº Array Interfaceï¼ˆæ•°ç»„æ¥å£ï¼‰åè®®ï¼š åè®®å¯¹æ¥ï¼š PILçš„Imageç±»å®ç°äº†__array_interface__å±æ€§ï¼Œä¸»åŠ¨å‘å¤–æš´éœ²å†…å­˜æŒ‡é’ˆå’Œå½¢çŠ¶ï¼› åº•å±‚è¯†åˆ«ï¼š torch.tensorçš„æ„é€ é€»è¾‘ä¼šè‡ªåŠ¨æ£€æµ‹æ­¤å±æ€§ï¼Œå®ç°ç±»ä¼¼NumPyçš„æ— ç¼æ•°æ®æ‹·è´ã€‚\n\n\n# è®¡ç®—æ¯ä¸ªæ•°å­—çš„åƒç´ å¹³å‡å€¼\n# 1. å †å å¼ é‡\n# 2. è½¬æˆfloatç±»å‹\n# 3. åŸºäºç»´åº¦0ï¼ˆå³æˆ‘ä»¬å †å å›¾ç‰‡çš„æ–¹å‘ï¼‰å–å¹³å‡å€¼\n# 4. å½’ä¸€åŒ–åƒç´ å€¼ï¼ˆ/255ï¼Œä½¿åƒç´ å€¼åˆ†å¸ƒåœ¨[0, 1]åŒºé—´ï¼‰\nmeans = [torch.stack(number).float().mean(0)/255 for number in training_nums]\n\n\n# æŸ¥çœ‹0-9åœ¨MNISTè®­ç»ƒé›†å¹³å‡åƒç´ ä¸‹çš„å›¾åƒ\nshow_images(means)\n\n\n\n\n\n\n\n\n\n#åŠ è½½æµ‹è¯•é›†\ntesting_paths = [testing_path/f'{i}' for i in range(10)]\ntesting_paths\n\n[Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/0'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/1'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/2'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/3'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/4'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/5'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/6'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/7'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/8'),\n Path('D:/User/ChenBo/.fastai/data/mnist_png/testing/9')]\n\n\n\n# å †å 10ä¸ªæ•°å­—å¹³å‡åƒç´ å›¾åƒ\nmean_stack = torch.stack(means)\nmean_stack.shape\n\ntorch.Size([10, 28, 28])\n\n\n\n#å®šä¹‰æŸå¤±å‡½æ•°\ndef mean_loss(preds, targets):\n    return (preds - targets.unsqueeze(1)).abs().mean((-1, -2)).argmin(1)\n\n\n# åŠ è½½æµ‹è¯•é›†\ntesting_nums = [[tensor(Image.open(image)) for image in path.ls()] for path in testing_paths]\nlen(testing_nums)\n\n10\n\n\n\n# è®¡ç®—éªŒè¯é›†çš„å‡†ç¡®åº¦\nsimilarity_accuracy = []\n\nfor i,test_num in enumerate(testing_nums):\n  curr = (mean_loss(mean_stack, torch.stack(test_num)) == i).float().mean()\n  similarity_accuracy.append(curr)\n  print(f\"accuracy of {i}: \", curr.item())\n\naccuracy of 0:  0.9132652878761292\naccuracy of 1:  0.9964757561683655\naccuracy of 2:  0.5145348906517029\naccuracy of 3:  0.7405940890312195\naccuracy of 4:  0.6537678241729736\naccuracy of 5:  0.22757847607135773\naccuracy of 6:  0.8486430048942566\naccuracy of 7:  0.7821011543273926\naccuracy of 8:  0.6529774069786072\naccuracy of 9:  0.8334985375404358\n\n\n\n# å¹³å‡å‡†ç¡®ç‡\ntorch.stack(similarity_accuracy).mean().item()\n\n0.7163436412811279\n\n\nå¯è§è¿™ä¹ˆç®€å•çš„ä¸€ä¸ªæ¨¡å‹ï¼ˆä¸éœ€è¦è®­ç»ƒï¼‰ï¼Œä»…é€šè¿‡åƒç´ å¹³å‡å€¼ï¼Œå‡†ç¡®ç‡å°±è¾¾åˆ°äº†71.63%ï¼Œä½œä¸ºæˆ‘ä»¬çš„åŸºåº§æ¨¡å‹ï¼Œæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚\nä½†æ˜¯å…¶ä½™æ¥è¿‘30%åˆ¤æ–­é”™è¯¯çš„å›¾åƒç©¶ç«Ÿæ˜¯é•¿ä»€ä¹ˆæ ·å‘¢ï¼Œä»¥è‡³äºå®¹æ˜“æ··æ·†ï¼Ÿè®©æˆ‘ä»¬ä¸€æ¢ç©¶ç«Ÿ\n\n# æŸ¥çœ‹testingæ•°æ®é›†ä¸­0çš„å›¾åƒ\nmean_loss(mean_stack, torch.stack(testing_nums[0]))\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 3, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 9, 6, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 8, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0,\n        0, 9, 0, 0, 0, 8, 0, 0, 0, 6, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 3, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 6, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 6, 9, 8, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 8, 0, 0, 0, 0,\n        6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 6, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0,\n        0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\nå¯ä»¥çœ‹åˆ°ç¬¬10å¼ å›¾ç‰‡ï¼ˆç´¢å¼•ä¸‹æ ‡ä¸º9ï¼‰è¢«è¯¯åˆ¤æˆäº†6,è®©æˆ‘ä»¬åŠ è½½ä¸‹è¿™å¼ å›¾ç‰‡çœ‹çœ‹\n\ntesting_zero = testing_paths[0].ls()\nimg_0_9 = Image.open(testing_zero[9])\nimg_0_9\n\n\n\n\n\n\n\n\nè¿™ä¸ª0ç¡®å®é•¿çš„æœ‰ç‚¹çŠ¯è§„ï¼Œæœ‰é‚£ä¹ˆä¸€ä¸¢ä¸¢åƒ6\nå†çœ‹ä¸‹ç¬¬13å¼ ï¼ˆç´¢å¼•ä¸‹æ ‡ä¸º12ï¼‰é•¿å¾—åƒ3çš„å›¾ç‰‡\n\nimg_0_12 = Image.open(testing_zero[12])\nimg_0_12\n\n\n\n\n\n\n\n\nè¿™ä¸ª0å°±å’Œ3æ²¡ä»€ä¹ˆå…³è”æ€§äº†ï¼Œå¯è§åŸºäºåƒç´ å¹³å‡å€¼æ˜¯ä¸€ä¸ªå¾ˆç²—ç³™çš„æ¨¡å‹æ„å»ºæ³•ã€‚\nä¸‹é¢è®©æˆ‘ä»¬æ­£å¼è¿›å…¥ç¥ç»ç½‘ç»œè®­ç»ƒï¼Œçœ‹çœ‹ç¥ç»ç½‘ç»œçš„é­”åŠ›å§ï¼\n\n\nç¥ç»ç½‘ç»œè®­ç»ƒ\n\nåŠ è½½æ•°æ®é›†DataLoaders\nfastaiä¸­çš„æ•°æ®é›†ç”±ä¸€ä¸ª(è®­ç»ƒé›†ï¼ŒéªŒè¯é›†)é€šè¿‡DataLoadersæ„é€ å‡½æ•°ï¼Œå³dls = DataLoaders(train_dl, test_dl)åˆ›å»º dlç”±DataSetæ‹†åˆ†æˆmini-batchäº§ç”Ÿ DataSetæ˜¯ç”±(æ¨¡å‹è¾“å…¥xï¼Œ å¯¹åº”çš„æ ‡ç­¾y)å…ƒç»„ç»„æˆçš„åˆ—è¡¨\n\n# æ„é€ æ¨¡å‹è¾“å…¥xï¼Œå³ç”±æ‰€æœ‰è®­ç»ƒé›†å›¾åƒçš„ä¸€ç»´å‘é‡ï¼ˆtensorï¼‰æ„æˆçš„list\ntraining_tensors = [tensor(Image.open(image)) for path in training_paths for image in path.ls()]\ntraining_stack = torch.stack(training_tensors).float() / 255\ntrain_x = training_stack.view(-1, 28 * 28)\ntrain_x.shape\n\ntorch.Size([60000, 784])\n\n\n\n# æ„é€ æ¨¡å‹è¾“å…¥å¯¹åº”çš„æ ‡ç­¾y\n# æ„å»ºæ ‡ç­¾yæœ‰ä¸¤ç§æ–¹æ¡ˆï¼Œä¸ç¥ç»ç½‘ç»œçš„å»ºæ¨¡ç›¸å¯¹åº”ã€‚\n#     1. ä½¿ç”¨one-hotç¼–ç ï¼Œå°†æ ‡ç­¾yç¼–ç æˆshape=[10]çš„ä¸€é˜¶å¼ é‡ï¼ˆå‘é‡ï¼‰ï¼Œç¥ç»ç½‘ç»œè¾“å‡ºè¦åšsoftmaxå½’ä¸€åŒ–å¤„ç†ã€‚\n#     2. ç›´æ¥ç¼–ç 0-9å¯¹åº”çš„æ•°å­—ï¼Œç¥ç»ç½‘ç»œè¾“å‡ºä¿æŒLogitsï¼Œä¸éœ€è¦åšsoftmaxï¼ŒæŸå¤±å‡½æ•°ä½¿ç”¨äº¤å‰ç†µã€‚\n# è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨æ–¹æ¡ˆäºŒ\ntrain_y = []\nfor i, path in enumerate(training_paths):\n    train_y.extend([i] * len(path.ls()))\n\ntrain_y = tensor(train_y)\ntrain_y.shape\n\ntorch.Size([60000])\n\n\n\n# æ„é€ è®­ç»ƒé›†dataloder\ntrain_ds = list(zip(train_x, train_y))\ntrain_dl = DataLoader(train_ds, batch_size=256)\n\n\n# å¦‚æ³•ç‚®åˆ¶æµ‹è¯•é›†åˆçš„dataloder\ntesting_tensors = [tensor(Image.open(image)) for path in testing_paths for image in path.ls()]\ntesting_stack = torch.stack(testing_tensors).float() / 255\ntest_x = testing_stack.view(-1, 28 * 28)\n\ntest_y = []\nfor i, path in enumerate(testing_paths):\n    test_y.extend([i] * len(path.ls()))\ntest_y = tensor(test_y)\n\ntest_ds = list(zip(test_x, test_y))\ntest_dl = DataLoader(test_ds, batch_size=256)\n\n\ndls = DataLoaders(train_dl, test_dl)\n\n\n\nåˆå§‹åŒ–æƒé‡\n\n# ä½¿ç”¨pytorchçš„nn.Sequentialæ¨¡å—å®šä¹‰æœ€ç®€å•çš„ç¥ç»ç½‘ç»œï¼Œç”±ä¸¤å±‚çº¿æ€§å±‚ã€ä¸€å±‚ReLUæ¿€æ´»å‡½æ•°ç»„æˆã€‚\n# åˆå§‹ç»´åº¦ä¸º28 * 28=784ï¼ˆæ¯ä¸€å¼ æ•°æ®é›†å›¾ç‰‡ç”±28 * 28çš„åƒç´ ç‚¹ç»„æˆï¼‰ï¼Œè¾“å‡ºç»´åº¦ä¸º10ï¼ˆåˆ†åˆ«ä»£è¡¨0-9ï¼‰ï¼Œä¸­é—´å±‚ç»´åº¦ä¸º30ï¼ˆä¸­é—´å±‚ç»´åº¦å¯ä»¥éšæ„è®¾ç½®ï¼Œè¡¨ç¤ºç¥ç»ç½‘ç»œå¯ä»¥æ•è·çš„ç‰¹å¾æ•°é‡ï¼‰ã€‚\n# æ¨¡å‹çš„æƒé‡ç”±nn.Linear()éšæœºåˆå§‹åŒ–ã€‚\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10)\n)\n\n\n\nå®šä¹‰æŸå¤±å‡½æ•°\näº¤å‰ç†µæŸå¤±å‡½æ•°ï¼šç›´æ¥ä½¿ç”¨CrossEntropyLossFlat()å‡½æ•°å³å¯ï¼Œç­‰åŒäºnn.CrossEntropyLossï¼ŒåŒºåˆ«åœ¨äºCrossEntropyLossFlatä¼šå±•å¹³inputså’Œtargets\n\n\nå®šä¹‰ä¼˜åŒ–å™¨\nä¼˜åŒ–å™¨é‡‡ç”¨éšæœºæ¢¯åº¦ä¸‹é™SGD\n\n\nå®šä¹‰æ­£ç¡®ç‡æŒ‡æ ‡\n\ndef batch_accuracy(preds, targets):\n    return (preds.argmax(1) == targets).float().mean()\n\n\n\nè®­ç»ƒç¥ç»ç½‘ç»œ\n\n\nlearn = Learner(\n    dls = dls,\n    model = simple_net,\n    loss_func=CrossEntropyLossFlat(),\n    opt_func=SGD,\n    metrics=batch_accuracy\n)\n\nlearn.remove_cbs(ProgressCallback)\n\n&lt;fastai.learner.Learner at 0x2ce446b1fd0&gt;\n\n\n\nlearn.fit_one_cycle(40, lr_max=0.1)\n\n[0, 2.058319330215454, 2.217005491256714, 0.1054999977350235, '00:00']\n[1, 1.026645541191101, 3.093644857406616, 0.10090000182390213, '00:00']\n[2, 0.46824803948402405, 3.1218841075897217, 0.10790000110864639, '00:00']\n[3, 0.2767549157142639, 3.022745132446289, 0.17020000517368317, '00:00']\n[4, 0.2017069160938263, 3.0426149368286133, 0.22300000488758087, '00:00']\n[5, 0.16536171734333038, 3.1070761680603027, 0.2531999945640564, '00:00']\n[6, 0.14760544896125793, 3.160271406173706, 0.27799999713897705, '00:00']\n[7, 0.13931012153625488, 3.1762616634368896, 0.29660001397132874, '00:00']\n[8, 0.1304607093334198, 3.268145799636841, 0.2971000075340271, '00:00']\n[9, 0.12770448625087738, 3.2473626136779785, 0.30469998717308044, '00:00']\n[10, 0.13348078727722168, 2.9868013858795166, 0.34529998898506165, '00:00']\n[11, 0.13116277754306793, 2.9267468452453613, 0.3483000099658966, '00:00']\n[12, 0.1418938934803009, 2.7756187915802, 0.3547999858856201, '00:00']\n[13, 0.14047884941101074, 2.686028242111206, 0.34880000352859497, '00:00']\n[14, 0.1492592990398407, 2.566951274871826, 0.3587999939918518, '00:00']\n[15, 0.14494456350803375, 2.585930824279785, 0.34380000829696655, '00:00']\n[16, 0.14443561434745789, 2.3903746604919434, 0.38370001316070557, '00:00']\n[17, 0.14076095819473267, 2.299257516860962, 0.40049999952316284, '00:00']\n[18, 0.1459321826696396, 2.113252639770508, 0.42570000886917114, '00:00']\n[19, 0.1465080976486206, 1.9186656475067139, 0.4675999879837036, '00:00']\n[20, 0.1467052400112152, 1.8600183725357056, 0.4738999903202057, '00:00']\n[21, 0.1455131471157074, 1.7872105836868286, 0.4810999929904938, '00:00']\n[22, 0.14577575027942657, 1.637719750404358, 0.49939998984336853, '00:00']\n[23, 0.14990916848182678, 1.477837085723877, 0.5246999859809875, '00:00']\n[24, 0.15500810742378235, 1.3052470684051514, 0.5654000043869019, '00:00']\n[25, 0.15911975502967834, 1.1460464000701904, 0.6098999977111816, '00:00']\n[26, 0.1634342521429062, 1.010521411895752, 0.6489999890327454, '00:00']\n[27, 0.16842171549797058, 0.9010326862335205, 0.6820999979972839, '00:00']\n[28, 0.1747148036956787, 0.7957713603973389, 0.7116000056266785, '00:00']\n[29, 0.1830490231513977, 0.691900908946991, 0.7461000084877014, '00:00']\n[30, 0.19352246820926666, 0.5899965167045593, 0.78329998254776, '00:00']\n[31, 0.2064102739095688, 0.4935455620288849, 0.8199999928474426, '00:00']\n[32, 0.22154438495635986, 0.407286673784256, 0.8560000061988831, '00:00']\n[33, 0.2384401112794876, 0.3356265425682068, 0.8860999941825867, '00:00']\n[34, 0.2557208836078644, 0.28089436888694763, 0.9089999794960022, '00:00']\n[35, 0.2705569863319397, 0.24455700814723969, 0.9248999953269958, '00:00']\n[36, 0.27901846170425415, 0.2253568172454834, 0.9315999746322632, '00:00']\n[37, 0.2801212668418884, 0.21798881888389587, 0.9340999722480774, '00:00']\n[38, 0.27724072337150574, 0.21609164774417877, 0.934499979019165, '00:00']\n[39, 0.27388298511505127, 0.21590690314769745, 0.934499979019165, '00:00']"
  },
  {
    "objectID": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#åŠ æ·±ç½‘ç»œ",
    "href": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#åŠ æ·±ç½‘ç»œ",
    "title": "åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ",
    "section": "åŠ æ·±ç½‘ç»œ",
    "text": "åŠ æ·±ç½‘ç»œ\n\nsimple_net2 = nn.Sequential(\n    nn.Linear(28*28, 128),\n    nn.ReLU(),\n    nn.Linear(128, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10)\n)\n\n\nlearn2 = Learner(\n    dls = dls,\n    model = simple_net2,\n    loss_func=CrossEntropyLossFlat(),\n    opt_func=SGD,\n    metrics=batch_accuracy\n)\n\nlearn2.remove_cbs(ProgressCallback)\n\n&lt;fastai.learner.Learner at 0x2ce443f2350&gt;\n\n\n\nlearn2.fit_one_cycle(40, lr_max=0.1)\n\n[0, 2.284194231033325, 2.2780888080596924, 0.10909999907016754, '00:00']\n[1, 1.7273132801055908, 3.121453285217285, 0.10090000182390213, '00:00']\n[2, 0.6533521413803101, 4.710193157196045, 0.10090000182390213, '00:00']\n[3, 0.3517478406429291, 4.200509071350098, 0.10300000011920929, '00:00']\n[4, 0.26247790455818176, 3.8570432662963867, 0.1111999973654747, '00:00']\n[5, 0.22667746245861053, 3.7390215396881104, 0.13210000097751617, '00:00']\n[6, 0.19849027693271637, 3.7495152950286865, 0.1736000031232834, '00:00']\n[7, 0.1698937714099884, 3.976940631866455, 0.1899999976158142, '00:00']\n[8, 0.16094760596752167, 3.9729621410369873, 0.24040000140666962, '00:00']\n[9, 0.15071767568588257, 4.124275207519531, 0.20829999446868896, '00:00']\n[10, 0.1586325764656067, 3.9294629096984863, 0.22370000183582306, '00:00']\n[11, 0.1524275243282318, 3.6151387691497803, 0.27469998598098755, '00:00']\n[12, 0.13830970227718353, 3.7923121452331543, 0.26330000162124634, '00:00']\n[13, 0.1439690738916397, 3.4316086769104004, 0.31200000643730164, '00:00']\n[14, 0.1358300894498825, 3.3017468452453613, 0.3206999897956848, '00:00']\n[15, 0.1349460333585739, 3.4888715744018555, 0.3122999966144562, '00:00']\n[16, 0.1303459256887436, 3.5109198093414307, 0.2890999913215637, '00:00']\n[17, 0.12825831770896912, 3.3352859020233154, 0.32919999957084656, '00:00']\n[18, 0.12744906544685364, 3.128218173980713, 0.3625999987125397, '00:00']\n[19, 0.13217177987098694, 2.9774820804595947, 0.38350000977516174, '00:00']\n[20, 0.13185277581214905, 2.9294333457946777, 0.38420000672340393, '00:00']\n[21, 0.12433288991451263, 2.8967883586883545, 0.4018000066280365, '00:00']\n[22, 0.1206083744764328, 2.7341668605804443, 0.4332999885082245, '00:00']\n[23, 0.13059327006340027, 2.306130886077881, 0.4700999855995178, '00:00']\n[24, 0.13621307909488678, 2.0357391834259033, 0.48420000076293945, '00:00']\n[25, 0.13968567550182343, 1.8700580596923828, 0.5041999816894531, '00:00']\n[26, 0.1441369503736496, 1.6961122751235962, 0.5260000228881836, '00:00']\n[27, 0.14897668361663818, 1.5259608030319214, 0.5483999848365784, '00:00']\n[28, 0.15509507060050964, 1.3486348390579224, 0.5794000029563904, '00:00']\n[29, 0.16277877986431122, 1.1601871252059937, 0.6193000078201294, '00:00']\n[30, 0.1735440194606781, 0.970603346824646, 0.6699000000953674, '00:00']\n[31, 0.1871323585510254, 0.7937489748001099, 0.7196000218391418, '00:00']\n[32, 0.2033500075340271, 0.6362507343292236, 0.7739999890327454, '00:00']\n[33, 0.22262418270111084, 0.5009270310401917, 0.8213000297546387, '00:00']\n[34, 0.24486522376537323, 0.38955897092819214, 0.8644000291824341, '00:00']\n[35, 0.26858794689178467, 0.30733853578567505, 0.8980000019073486, '00:00']\n[36, 0.28942427039146423, 0.25650855898857117, 0.9186999797821045, '00:00']\n[37, 0.2997584640979767, 0.23445925116539001, 0.928600013256073, '00:00']\n[38, 0.2956204414367676, 0.22947126626968384, 0.9305999875068665, '00:00']\n[39, 0.2885822057723999, 0.22912432253360748, 0.9309999942779541, '00:00']\n\n\nå¯ä»¥çœ‹å‡ºï¼Œå¢åŠ ä¸€å±‚ç½‘ç»œå®é™…ä¸Šå¹¶æ²¡æœ‰æé«˜å‡†ç¡®ç‡ï¼Œåè€Œç•¥å¾®ä¸‹é™ï¼Œä¸€ä¸ªåŸå› æ˜¯ç½‘ç»œå˜æ·±äº†ä½†æ˜¯å‚æ•°é‡å·®åˆ«ä¸å¤§"
  },
  {
    "objectID": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#ä½¿ç”¨resnet18",
    "href": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#ä½¿ç”¨resnet18",
    "title": "åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ",
    "section": "ä½¿ç”¨RESNET18",
    "text": "ä½¿ç”¨RESNET18\n\npath = untar_data(URLs.MNIST)\n\n\ndls = ImageDataLoaders.from_folder(path,train=\"training\",valid=\"testing\")\n\n\nlearn = vision_learner(dls, resnet18, pretrained=False,\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.remove_cbs(ProgressCallback)\n\n&lt;fastai.learner.Learner at 0x2ce5fe02ad0&gt;\n\n\n\nlearn.fit_one_cycle(5, 0.1)\n\n[0, 1.1184285879135132, 791.0833129882812, 0.8259999752044678, '00:32']\n[1, 0.929822564125061, 3.3093323707580566, 0.9657999873161316, '00:32']\n[2, 0.4890580475330353, 1.0338960886001587, 0.9674000144004822, '00:32']\n[3, 0.08392318338155746, 0.047650303691625595, 0.989799976348877, '00:32']\n[4, 0.026931514963507652, 0.021246496587991714, 0.9937999844551086, '00:32']\n\n\næ­£ç¡®ç‡ç›´æ¥é£™åˆ°99.38%ï¼ŒAmazingï¼ï¼ï¼\n\næŸ¥çœ‹æ··æ·†çŸ©é˜µ\næœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æŸå¤±æœ€é«˜çš„å›¾åƒé•¿ä»€ä¹ˆæ ·ï¼Œæ˜¯å¦ç¡®å®éš¾ä»¥åˆ†è¾¨\n\nfrom fastai.vision.widgets import *\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n# æŸ¥çœ‹lossæœ€é«˜çš„5å¼ å›¾ç‰‡ï¼Œå¯ä»¥çœ‹åˆ°ç¡®å®éƒ½æŒºéš¾è¾¨è®¤çš„\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\n\n\n\n\n\n# ä»¥è®­ç»ƒé›†ä¸­çš„`0`ä¸ºä¾‹ï¼Œè¿™äº›å°±ç®—æ˜¯äººç±»ä¹Ÿéš¾ä»¥åˆ†è¾¨æ•°å­—æ˜¯0\ncleaner = ImageClassifierCleaner(learn)\ncleaner"
  },
  {
    "objectID": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#å°ç»“",
    "href": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#å°ç»“",
    "title": "åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ",
    "section": "å°ç»“",
    "text": "å°ç»“\nMNISTæ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒä½œä¸ºæ·±åº¦å­¦ä¹ é¢†åŸŸçš„HelloWorldï¼Œæƒ³è¦ç†è§£å¹¶è·‘é€šå¹¶ä¸æ˜¯ä¸€ä»¶ç®€å•çš„äº‹ã€‚ æ­£å¦‚Fastaiåˆ›å§‹äººJeremy Howardæ‰€è¯´ï¼Œè¦æƒ³æ·±å…¥æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œå­¦ä¹ è€…å¿…é¡»ä¿æŒåšéŸ§ã€‚ æ¯”å¦‚å¯¹äºæˆ‘æ¥è¯´ï¼š 2å°æ—¶çš„è¯¾ç¨‹å¯èƒ½å¯¹åº”äºè§†é¢‘è¯¾ç¨‹çœ‹3éï¼Œjupyter notebookææ–™çœ‹3éï¼Œä¸è®¡å…¶æ•°çš„æŸ¥è¯¢ä¸æœç´¢ï¼Œä¸€æ¬¡æ¬¡çš„æš‚åœè¯¾ä»¶å¹¶å¡ä½ï¼Œæ‰å ªå ªç†è§£å¹¶å®Œæˆè¿™ç¯‡åšå®¢ã€‚\nä½†æ˜¯å¦‚ä½ æ‰€è§ï¼Œæœ€åæ¨¡å‹çš„å‡†ç¡®ç‡æ˜¯æƒŠäººçš„ã€‚åŸºåº§æ¨¡å‹71.63%-&gt;å•å±‚ç¥ç»ç½‘ç»œ93.45%-&gt;resnet18 99.38%ï¼Œè€Œresnet18ä¸è¿‡æ˜¯ä¸€ä¸ª2015å¹´çš„æ—©å·²è¿‡æ—¶çš„æ¨¡å‹ã€‚ å…¶ä¸­çš„æ¯ä¸€æ­¥éƒ½ç¯ç¯ç›¸æ‰£ï¼Œç„¶è€Œè¿™åªæ˜¯é­”æœ¯ï¼Œå¹¶ä¸æ˜¯é­”æ³•ï¼Œä¸€ä¸ªäººäººéƒ½å¯ä»¥åšåˆ°çš„é­”æœ¯è¡¨æ¼”ã€‚"
  },
  {
    "objectID": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#references",
    "href": "posts/åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ/index.html#references",
    "title": "åŸºäºMNISTå…¨é‡æ•°æ®é›†çš„ç¥ç»ç½‘ç»œè®­ç»ƒ",
    "section": "References",
    "text": "References\n\nfastaiå¯¹åº”è¯¾ç¨‹é“¾æ¥"
  },
  {
    "objectID": "08_collab.html",
    "href": "08_collab.html",
    "title": "Collaborative Filtering Deep Dive",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\n[[chapter_collab]]"
  },
  {
    "objectID": "08_collab.html#a-first-look-at-the-data",
    "href": "08_collab.html#a-first-look-at-the-data",
    "title": "Collaborative Filtering Deep Dive",
    "section": "A First Look at the Data",
    "text": "A First Look at the Data\nWe do not have access to Netflixâ€™s entire dataset of movie watching history, but there is a great dataset that we can use, called MovieLens. This dataset contains tens of millions of movie rankings (a combination of a movie ID, a user ID, and a numeric rating), although we will just use a subset of 100,000 of them for our example. If youâ€™re interested, it would be a great learning project to try and replicate this approach on the full 25-million recommendation dataset, which you can get from their website.\nThe dataset is available through the usual fastai function:\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\nAccording to the README, the main table is in the file u.data. It is tab-separated and the columns are, respectively user, movie, rating, and timestamp. Since those names are not encoded, we need to indicate them when reading the file with Pandas. Here is a way to open this table and take a look:\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n                      names=['user','movie','rating','timestamp'])\nratings.head()\n\n\n    \n\n\n\n\n\n\nuser\nmovie\nrating\ntimestamp\n\n\n\n\n0\n196\n242\n3\n881250949\n\n\n1\n186\n302\n3\n891717742\n\n\n2\n22\n377\n1\n878887116\n\n\n3\n244\n51\n2\n880606923\n\n\n4\n166\n346\n1\n886397596\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\nAlthough this has all the information we need, it is not a particularly helpful way for humans to look at this data. &lt;&gt; shows the same data cross-tabulated into a human-friendly table.\n\nWe have selected just a few of the most popular movies, and users who watch the most movies, for this crosstab example. The empty cells in this table are the things that we would like our model to learn to fill in. Those are the places where a user has not reviewed the movie yet, presumably because they have not watched it. For each user, we would like to figure out which of those movies they might be most likely to enjoy.\nIf we knew for each user to what degree they liked each important category that a movie might fall into, such as genre, age, preferred directors and actors, and so forth, and we knew the same information about each movie, then a simple way to fill in this table would be to multiply this information together for each movie and use a combination. For instance, assuming these factors range between -1 and +1, with positive numbers indicating stronger matches and negative numbers weaker ones, and the categories are science-fiction, action, and old movies, then we could represent the movie The Last Skywalker as:\n\nlast_skywalker = np.array([0.98,0.9,-0.9])\n\nHere, for instance, we are scoring very science-fiction as 0.98, very action as 0.9, and very not old as -0.9. We could represent a user who likes modern sci-fi action movies as:\n\nuser1 = np.array([0.9,0.8,-0.6])\n\nand we can now calculate the match between this combination:\n\n(user1*last_skywalker).sum()\n\n2.1420000000000003\n\n\nWhen we multiply two vectors together and add up the results, this is known as the dot product. It is used a lot in machine learning, and forms the basis of matrix multiplication. We will be looking a lot more at matrix multiplication and dot products in &lt;&gt;.\n\njargon: dot product: The mathematical operation of multiplying the elements of two vectors together, and then summing up the result.\n\nOn the other hand, we might represent the movie Casablanca as:\n\ncasablanca = np.array([-0.99,-0.3,0.8])\n\nThe match between this combination is:\n\n(user1*casablanca).sum()\n\n-1.611\n\n\nSince we donâ€™t know what the latent factors actually are, and we donâ€™t know how to score them for each user and movie, we should learn them."
  },
  {
    "objectID": "08_collab.html#learning-the-latent-factors",
    "href": "08_collab.html#learning-the-latent-factors",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Learning the Latent Factors",
    "text": "Learning the Latent Factors\nThere is surprisingly little difference between specifying the structure of a model, as we did in the last section, and learning one, since we can just use our general gradient descent approach.\nStep 1 of this approach is to randomly initialize some parameters. These parameters will be a set of latent factors for each user and movie. We will have to decide how many to use. We will discuss how to select this shortly, but for illustrative purposes letâ€™s use 5 for now. Because each user will have a set of these factors and each movie will have a set of these factors, we can show these randomly initialized values right next to the users and movies in our crosstab, and we can then fill in the dot products for each of these combinations in the middle. For example, &lt;&gt; shows what it looks like in Microsoft Excel, with the top-left cell formula displayed as an example.\n\nStep 2 of this approach is to calculate our predictions. As weâ€™ve discussed, we can do this by simply taking the dot product of each movie with each user. If, for instance, the first latent user factor represents how much the user likes action movies and the first latent movie factor represents if the movie has a lot of action or not, the product of those will be particularly high if either the user likes action movies and the movie has a lot of action in it or the user doesnâ€™t like action movies and the movie doesnâ€™t have any action in it. On the other hand, if we have a mismatch (a user loves action movies but the movie isnâ€™t an action film, or the user doesnâ€™t like action movies and it is one), the product will be very low.\nStep 3 is to calculate our loss. We can use any loss function that we wish; letâ€™s pick mean squared error for now, since that is one reasonable way to represent the accuracy of a prediction.\nThatâ€™s all we need. With this in place, we can optimize our parameters (that is, the latent factors) using stochastic gradient descent, such as to minimize the loss. At each step, the stochastic gradient descent optimizer will calculate the match between each movie and each user using the dot product, and will compare it to the actual rating that each user gave to each movie. It will then calculate the derivative of this value and will step the weights by multiplying this by the learning rate. After doing this lots of times, the loss will get better and better, and the recommendations will also get better and better.\nTo use the usual Learner.fit function we will need to get our data into a DataLoaders, so letâ€™s focus on that now."
  },
  {
    "objectID": "08_collab.html#creating-the-dataloaders",
    "href": "08_collab.html#creating-the-dataloaders",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Creating the DataLoaders",
    "text": "Creating the DataLoaders\nWhen showing the data, we would rather see movie titles than their IDs. The table u.item contains the correspondence of IDs to titles:\n\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n                     usecols=(0,1), names=('movie','title'), header=None)\nmovies.head()\n\n\n\n\n\n\n\n\nmovie\ntitle\n\n\n\n\n0\n1\nToy Story (1995)\n\n\n1\n2\nGoldenEye (1995)\n\n\n2\n3\nFour Rooms (1995)\n\n\n3\n4\nGet Shorty (1995)\n\n\n4\n5\nCopycat (1995)\n\n\n\n\n\n\n\nWe can merge this with our ratings table to get the user ratings by title:\n\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n\n\n\nuser\nmovie\nrating\ntimestamp\ntitle\n\n\n\n\n0\n196\n242\n3\n881250949\nKolya (1996)\n\n\n1\n63\n242\n3\n875747190\nKolya (1996)\n\n\n2\n226\n242\n5\n883888671\nKolya (1996)\n\n\n3\n154\n242\n3\n879138235\nKolya (1996)\n\n\n4\n306\n242\n5\n876503793\nKolya (1996)\n\n\n\n\n\n\n\nWe can then build a DataLoaders object from this table. By default, it takes the first column for the user, the second column for the item (here our movies), and the third column for the ratings. We need to change the value of item_name in our case to use the titles instead of the IDs:\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n\n\nuser\ntitle\nrating\n\n\n\n\n0\n542\nMy Left Foot (1989)\n4\n\n\n1\n422\nEvent Horizon (1997)\n3\n\n\n2\n311\nAfrican Queen, The (1951)\n4\n\n\n3\n595\nFace/Off (1997)\n4\n\n\n4\n617\nEvil Dead II (1987)\n1\n\n\n5\n158\nJurassic Park (1993)\n5\n\n\n6\n836\nChasing Amy (1997)\n3\n\n\n7\n474\nEmma (1996)\n3\n\n\n8\n466\nJackie Chan's First Strike (1996)\n3\n\n\n9\n554\nScream (1996)\n3\n\n\n\n\n\nTo represent collaborative filtering in PyTorch we canâ€™t just use the crosstab representation directly, especially if we want it to fit into our deep learning framework. We can represent our movie and user latent factor tables as simple matrices:\n\ndls.classes\n\n{'user': (#944) ['#na#',1,2,3,4,5,6,7,8,9...],\n 'title': (#1635) ['#na#',\"'Til There Was You (1997)\",'1-900 (1994)','101 Dalmatians (1996)','12 Angry Men (1957)','187 (1997)','2 Days in the Valley (1996)','20,000 Leagues Under the Sea (1954)','2001: A Space Odyssey (1968)','3 Ninjas: High Noon At Mega Mountain (1998)'...]}\n\n\n\nn_users  = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nTo calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. But look up in an index is not an operation our deep learning models know how to do. They know how to do matrix products, and activation functions.\nFortunately, it turns out that we can represent look up in an index as a matrix product. The trick is to replace our indices with one-hot-encoded vectors. Here is an example of what happens if we multiply a vector by a one-hot-encoded vector representing the index 3:\n\none_hot_3 = one_hot(3, n_users).float()\n\n\nuser_factors.t() @ one_hot_3\n\ntensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908])\n\n\nIt gives us the same vector as the one at index 3 in the matrix:\n\nuser_factors[3]\n\ntensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908])\n\n\nIf we do that for a few indices at once, we will have a matrix of one-hot-encoded vectors, and that operation will be a matrix multiplication! This would be a perfectly acceptable way to build models using this kind of architecture, except that it would use a lot more memory and time than necessary. We know that there is no real underlying reason to store the one-hot-encoded vector, or to search through it to find the occurrence of the number oneâ€”we should just be able to index into an array directly with an integer. Therefore, most deep learning libraries, including PyTorch, include a special layer that does just this; it indexes into a vector using an integer, but has its derivative calculated in such a way that it is identical to what it would have been if it had done a matrix multiplication with a one-hot-encoded vector. This is called an embedding.\n\njargon: Embedding: Multiplying by a one-hot-encoded matrix, using the computational shortcut that it can be implemented by simply indexing directly. This is quite a fancy word for a very simple concept. The thing that you multiply the one-hot-encoded matrix by (or, using the computational shortcut, index into directly) is called the embedding matrix.\n\nIn computer vision, we have a very easy way to get all the information of a pixel through its RGB values: each pixel in a colored image is represented by three numbers. Those three numbers give us the redness, the greenness and the blueness, which is enough to get our model to work afterward.\nFor the problem at hand, we donâ€™t have the same easy way to characterize a user or a movie. There are probably relations with genres: if a given user likes romance, they are likely to give higher scores to romance movies. Other factors might be whether the movie is more action-oriented versus heavy on dialogue, or the presence of a specific actor that a user might particularly like.\nHow do we determine numbers to characterize those? The answer is, we donâ€™t. We will let our model learn them. By analyzing the existing relations between users and movies, our model can figure out itself the features that seem important or not.\nThis is what embeddings are. We will attribute to each of our users and each of our movies a random vector of a certain length (here, n_factors=5), and we will make those learnable parameters. That means that at each step, when we compute the loss by comparing our predictions to our targets, we will compute the gradients of the loss with respect to those embedding vectors and update them with the rules of SGD (or another optimizer).\nAt the beginning, those numbers donâ€™t mean anything since we have chosen them randomly, but by the end of training, they will. By learning on existing data about the relations between users and movies, without having any other information, we will see that they still get some important features, and can isolate blockbusters from independent cinema, action movies from romance, and so on.\nWe are now in a position that we can create our whole model from scratch."
  },
  {
    "objectID": "08_collab.html#collaborative-filtering-from-scratch",
    "href": "08_collab.html#collaborative-filtering-from-scratch",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Collaborative Filtering from Scratch",
    "text": "Collaborative Filtering from Scratch\nBefore we can write a model in PyTorch, we first need to learn the basics of object-oriented programming and Python. If you havenâ€™t done any object-oriented programming before, we will give you a quick introduction here, but we would recommend looking up a tutorial and getting some practice before moving on.\nThe key idea in object-oriented programming is the class. We have been using classes throughout this book, such as DataLoader, string, and Learner. Python also makes it easy for us to create new classes. Here is an example of a simple class:\n\nclass Example:\n    def __init__(self, a): self.a = a\n    def say(self,x): return f'Hello {self.a}, {x}.'\n\nThe most important piece of this is the special method called __init__ (pronounced dunder init). In Python, any method surrounded in double underscores like this is considered special. It indicates that there is some extra behavior associated with this method name. In the case of __init__, this is the method Python will call when your new object is created. So, this is where you can set up any state that needs to be initialized upon object creation. Any parameters included when the user constructs an instance of your class will be passed to the __init__ method as parameters. Note that the first parameter to any method defined inside a class is self, so you can use this to set and get any attributes that you will need:\n\nex = Example('Sylvain')\nex.say('nice to meet you')\n\n'Hello Sylvain, nice to meet you.'\n\n\nAlso note that creating a new PyTorch module requires inheriting from Module. Inheritance is an important object-oriented concept that we will not discuss in detail hereâ€”in short, it means that we can add additional behavior to an existing class. PyTorch already provides a Module class, which provides some basic foundations that we want to build on. So, we add the name of this superclass after the name of the class that we are defining, as shown in the following example.\nThe final thing that you need to know to create a new PyTorch module is that when your module is called, PyTorch will call a method in your class called forward, and will pass along to that any parameters that are included in the call. Here is the class defining our dot product model:\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n\n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return (users * movies).sum(dim=1)\n\nIf you havenâ€™t seen object-oriented programming before, then donâ€™t worry, you wonâ€™t need to use it much in this book. We are just mentioning this approach here, because most online tutorials and documentation will use the object-oriented syntax.\nNote that the input of the model is a tensor of shape batch_size x 2, where the first column (x[:, 0]) contains the user IDs and the second column (x[:, 1]) contains the movie IDs. As explained before, we use the embedding layers to represent our matrices of user and movie latent factors:\n\nx,y = dls.one_batch()\nx.shape\n\ntorch.Size([64, 2])\n\n\nNow that we have defined our architecture, and created our parameter matrices, we need to create a Learner to optimize our model. In the past we have used special functions, such as vision_learner, which set up everything for us for a particular application. Since we are doing things from scratch here, we will use the plain Learner class:\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nWe are now ready to fit our model:\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.993168\n0.990168\n00:12\n\n\n1\n0.884821\n0.911269\n00:12\n\n\n2\n0.671865\n0.875679\n00:12\n\n\n3\n0.471727\n0.878200\n00:11\n\n\n4\n0.361314\n0.884209\n00:12\n\n\n\n\n\nThe first thing we can do to make this model a little bit better is to force those predictions to be between 0 and 5. For this, we just need to use sigmoid_range, like in &lt;&gt;. One thing we discovered empirically is that itâ€™s better to have the range go a little bit over 5, so we use (0, 5.5):\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.973745\n0.993206\n00:12\n\n\n1\n0.869132\n0.914323\n00:12\n\n\n2\n0.676553\n0.870192\n00:12\n\n\n3\n0.485377\n0.873865\n00:12\n\n\n4\n0.377866\n0.877610\n00:11\n\n\n\n\n\nThis is a reasonable start, but we can do better. One obvious missing piece is that some users are just more positive or negative in their recommendations than others, and some movies are just plain better or worse than others. But in our dot product representation we do not have any way to encode either of these things. If all you can say about a movie is, for instance, that it is very sci-fi, very action-oriented, and very not old, then you donâ€™t really have any way to say whether most people like it.\nThatâ€™s because at this point we only have weights; we do not have biases. If we have a single number for each user that we can add to our scores, and ditto for each movie, that will handle this missing piece very nicely. So first of all, letâ€™s adjust our model architecture:\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(res, *self.y_range)\n\nLetâ€™s try training this and see how it goes:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.929161\n0.936303\n00:13\n\n\n1\n0.820444\n0.861306\n00:13\n\n\n2\n0.621612\n0.865306\n00:14\n\n\n3\n0.404648\n0.886448\n00:13\n\n\n4\n0.292948\n0.892580\n00:13\n\n\n\n\n\nInstead of being better, it ends up being worse (at least at the end of training). Why is that? If we look at both trainings carefully, we can see the validation loss stopped improving in the middle and started to get worse. As weâ€™ve seen, this is a clear indication of overfitting. In this case, there is no way to use data augmentation, so we will have to use another regularization technique. One approach that can be helpful is weight decay.\n\nWeight Decay\nWeight decay, or L2 regularization, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\nWhy would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, y = a * (x**2), the larger a is, the more narrow the parabola is (&lt;&gt;).\n\n#hide_input\n#id parabolas\nx = np.linspace(-2,2,100)\na_s = [1,2,5,10,50]\nys = [a * x**2 for a in a_s]\n_,ax = plt.subplots(figsize=(8,6))\nfor a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')\nax.set_ylim([0,5])\nax.legend();\n\n\n\n\n\n\n\n\nSo, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting.\nLimiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just wd) is a parameter that controls that sum of squares we add to our loss (assuming parameters is a tensor of all parameters):\nloss_with_wd = loss + wd * (parameters**2).sum()\nIn practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of p**2 with respect to p is 2*p, so adding that big sum to our loss is exactly the same as doing:\nparameters.grad += wd * 2 * parameters\nIn practice, since wd is a parameter that we choose, we can just make it twice as big, so we donâ€™t even need the *2 in this equation. To use weight decay in fastai, just pass wd in your call to fit or fit_one_cycle:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.972090\n0.962366\n00:13\n\n\n1\n0.875591\n0.885106\n00:13\n\n\n2\n0.723798\n0.839880\n00:13\n\n\n3\n0.586002\n0.823225\n00:13\n\n\n4\n0.490980\n0.823060\n00:13\n\n\n\n\n\nMuch better!\n\n\nCreating Our Own Embedding Module\nSo far, weâ€™ve used Embedding without thinking about how it really works. Letâ€™s re-create DotProductBias without using this class. Weâ€™ll need a randomly initialized weight matrix for each of the embeddings. We have to be careful, however. Recall from &lt;&gt; that optimizers require that they can get all the parameters of a module from the moduleâ€™s parameters method. However, this does not happen fully automatically. If we just add a tensor as an attribute to a Module, it will not be included in parameters:\n\nclass T(Module):\n    def __init__(self): self.a = torch.ones(3)\n\nL(T().parameters())\n\n(#0) []\n\n\nTo tell Module that we want to treat a tensor as a parameter, we have to wrap it in the nn.Parameter class. This class doesnâ€™t actually add any functionality (other than automatically calling requires_grad_ for us). Itâ€™s only used as a â€œmarkerâ€ to show what to include in parameters:\n\nclass T(Module):\n    def __init__(self): self.a = nn.Parameter(torch.ones(3))\n\nL(T().parameters())\n\n(#1) [Parameter containing:\ntensor([1., 1., 1.], requires_grad=True)]\n\n\nAll PyTorch modules use nn.Parameter for any trainable parameters, which is why we havenâ€™t needed to explicitly use this wrapper up until now:\n\nclass T(Module):\n    def __init__(self): self.a = nn.Linear(1, 3, bias=False)\n\nt = T()\nL(t.parameters())\n\n(#1) [Parameter containing:\ntensor([[-0.9595],\n        [-0.8490],\n        [ 0.8159]], requires_grad=True)]\n\n\n\ntype(t.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\nWe can create a tensor as a parameter, with random initialization, like so:\n\ndef create_params(size):\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n\nLetâ€™s use this to create DotProductBias again, but without Embedding:\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        res = (users*movies).sum(dim=1)\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(res, *self.y_range)\n\nThen letâ€™s train it again to check we get around the same results we saw in the previous section:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.962146\n0.936952\n00:14\n\n\n1\n0.858084\n0.884951\n00:14\n\n\n2\n0.740883\n0.838549\n00:14\n\n\n3\n0.592497\n0.823599\n00:14\n\n\n4\n0.473570\n0.824263\n00:14\n\n\n\n\n\nNow, letâ€™s take a look at what our model has learned."
  },
  {
    "objectID": "08_collab.html#interpreting-embeddings-and-biases",
    "href": "08_collab.html#interpreting-embeddings-and-biases",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Interpreting Embeddings and Biases",
    "text": "Interpreting Embeddings and Biases\nOur model is already useful, in that it can provide us with movie recommendations for our usersâ€”but it is also interesting to see what parameters it has discovered. The easiest to interpret are the biases. Here are the movies with the lowest values in the bias vector:\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n 'Beautician and the Beast, The (1997)',\n 'Crow: City of Angels, The (1996)',\n 'Home Alone 3 (1997)']\n\n\nThink about what this means. What itâ€™s saying is that for each of these movies, even when a user is very well matched to its latent factors (which, as we will see in a moment, tend to represent things like level of action, age of movie, and so forth), they still generally donâ€™t like it. We could have simply sorted the movies directly by their average rating, but looking at the learned bias tells us something much more interesting. It tells us not just whether a movie is of a kind that people tend not to enjoy watching, but that people tend not to like watching it even if it is of a kind that they would otherwise enjoy! By the same token, here are the movies with the highest bias:\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['L.A. Confidential (1997)',\n 'Titanic (1997)',\n 'Silence of the Lambs, The (1991)',\n 'Shawshank Redemption, The (1994)',\n 'Star Wars (1977)']\n\n\nSo, for instance, even if you donâ€™t normally enjoy detective movies, you might enjoy LA Confidential!\nIt is not quite so easy to directly interpret the embedding matrices. There are just too many factors for a human to look at. But there is a technique that can pull out the most important underlying directions in such a matrix, called principal component analysis (PCA). We will not be going into this in detail in this book, because it is not particularly important for you to understand to be a deep learning practitioner, but if you are interested then we suggest you check out the fast.ai course Computational Linear Algebra for Coders. &lt;&gt; shows what our movies look like based on two of the strongest PCA components.\n\n#hide_input\n#id img_pca_movie\n#caption Representation of movies based on two strongest PCA components\n#alt Representation of movies based on two strongest PCA components\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see here that the model seems to have discovered a concept of classic versus pop culture movies, or perhaps it is critically acclaimed that is represented here.\n\nj: No matter how many models I train, I never stop getting moved and surprised by how these randomly initialized bunches of numbers, trained with such simple mechanics, manage to discover things about my data all by themselves. It almost seems like cheating, that I can create code that does useful things without ever actually telling it how to do those things!\n\nWe defined our model from scratch to teach you what is inside, but you can directly use the fastai library to build it. Weâ€™ll look at how to do that next.\n\nUsing fastai.collab\nWe can create and train a collaborative filtering model using the exact structure shown earlier by using fastaiâ€™s collab_learner:\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.931751\n0.953806\n00:13\n\n\n1\n0.851826\n0.878119\n00:13\n\n\n2\n0.715254\n0.834711\n00:13\n\n\n3\n0.583173\n0.821470\n00:13\n\n\n4\n0.496625\n0.821688\n00:13\n\n\n\n\n\nThe names of the layers can be seen by printing the model:\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1635, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1635, 1)\n)\n\n\nWe can use these to replicate any of the analyses we did in the previous sectionâ€”for instance:\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n \"Schindler's List (1993)\",\n 'Shawshank Redemption, The (1994)',\n 'L.A. Confidential (1997)',\n 'Silence of the Lambs, The (1991)']\n\n\nAnother interesting thing we can do with these learned embeddings is to look at distance.\n\n\nEmbedding Distance\nOn a two-dimensional map we can calculate the distance between two coordinates using the formula of Pythagoras: \\(\\sqrt{x^{2}+y^{2}}\\) (assuming that x and y are the distances between the coordinates on each axis). For a 50-dimensional embedding we can do exactly the same thing, except that we add up the squares of all 50 of the coordinate distances.\nIf there were two movies that were nearly identical, then their embedding vectors would also have to be nearly identical, because the users that would like them would be nearly exactly the same. There is a more general idea here: movie similarity can be defined by the similarity of users that like those movies. And that directly means that the distance between two moviesâ€™ embedding vectors can define that similarity. We can use this to find the most similar movie to Silence of the Lambs:\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = distances.argsort(descending=True)[1]\ndls.classes['title'][idx]\n\n'Dial M for Murder (1954)'\n\n\nNow that we have succesfully trained a model, letâ€™s see how to deal with the situation where we have no data for a user. How can we make recommendations to new users?"
  },
  {
    "objectID": "08_collab.html#bootstrapping-a-collaborative-filtering-model",
    "href": "08_collab.html#bootstrapping-a-collaborative-filtering-model",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Bootstrapping a Collaborative Filtering Model",
    "text": "Bootstrapping a Collaborative Filtering Model\nThe biggest challenge with using collaborative filtering models in practice is the bootstrapping problem. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user?\nBut even if you are a well-established company with a long history of user transactions, you still have the question: what do you do when a new user signs up? And indeed, what do you do when you add a new product to your portfolio? There is no magic solution to this problem, and really the solutions that we suggest are just variations of use your common sense. You could assign new users the mean of all of the embedding vectors of your other users, but this has the problem that that particular combination of latent factors may be not at all common (for instance, the average for the science-fiction factor may be high, and the average for the action factor may be low, but it is not that common to find people who like science-fiction without action). Better would probably be to pick some particular user to represent average taste.\nBetter still is to use a tabular model based on user meta data to construct your initial embedding vector. When a user signs up, think about what questions you could ask them that could help you to understand their tastes. Then you can create a model where the dependent variable is a userâ€™s embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup metadata. We will see in the next section how to create these kinds of tabular models. (You may have noticed that when you sign up for services such as Pandora and Netflix, they tend to ask you a few questions about what genres of movie or music you like; this is how they come up with your initial collaborative filtering recommendations.)\nOne thing to be careful of is that a small number of extremely enthusiastic users may end up effectively setting the recommendations for your whole user base. This is a very common problem, for instance, in movie recommendation systems. People that watch anime tend to watch a whole lot of it, and donâ€™t watch very much else, and spend a lot of time putting their ratings on websites. As a result, anime tends to be heavily overrepresented in a lot of best ever movies lists. In this particular case, it can be fairly obvious that you have a problem of representation bias, but if the bias is occurring in the latent factors then it may not be obvious at all.\nSuch a problem can change the entire makeup of your user base, and the behavior of your system. This is particularly true because of positive feedback loops. If a small number of your users tend to set the direction of your recommendation system, then they are naturally going to end up attracting more people like them to your system. And that will, of course, amplify the original representation bias. This type of bias has a natural tendency to be amplified exponentially. You may have seen examples of company executives expressing surprise at how their online platforms rapidly deteriorated in such a way that they expressed values at odds with the values of the founders. In the presence of these kinds of feedback loops, it is easy to see how such a divergence can happen both quickly and in a way that is hidden until it is too late.\nIn a self-reinforcing system like this, we should probably expect these kinds of feedback loops to be the norm, not the exception. Therefore, you should assume that you will see them, plan for that, and identify up front how you will deal with these issues. Try to think about all of the ways in which feedback loops may be represented in your system, and how you might be able to identify them in your data. In the end, this is coming back to our original advice about how to avoid disaster when rolling out any kind of machine learning system. Itâ€™s all about ensuring that there are humans in the loop; that there is careful monitoring, and a gradual and thoughtful rollout.\nOur dot product model works quite well, and it is the basis of many successful real-world recommendation systems. This approach to collaborative filtering is known as probabilistic matrix factorization (PMF). Another approach, which generally works similarly well given the same data, is deep learning."
  },
  {
    "objectID": "08_collab.html#deep-learning-for-collaborative-filtering",
    "href": "08_collab.html#deep-learning-for-collaborative-filtering",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Deep Learning for Collaborative Filtering",
    "text": "Deep Learning for Collaborative Filtering\nTo turn our architecture into a deep learning model, the first step is to take the results of the embedding lookup and concatenate those activations together. This gives us a matrix which we can then pass through linear layers and nonlinearities in the usual way.\nSince weâ€™ll be concatenating the embeddings, rather than taking their dot product, the two embedding matrices can have different sizes (i.e., different numbers of latent factors). fastai has a function get_emb_sz that returns recommended sizes for embedding matrices for your data, based on a heuristic that fast.ai has found tends to work well in practice:\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1635, 101)]\n\n\nLetâ€™s implement this class:\n\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act),\n            nn.ReLU(),\n            nn.Linear(n_act, 1))\n        self.y_range = y_range\n\n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n\nAnd use it to create a model:\n\nmodel = CollabNN(*embs)\n\nCollabNN creates our Embedding layers in the same way as previous classes in this chapter, except that we now use the embs sizes. self.layers is identical to the mini-neural net we created in &lt;&gt; for MNIST. Then, in forward, we apply the embeddings, concatenate the results, and pass this through the mini-neural net. Finally, we apply sigmoid_range as we have in previous models.\nLetâ€™s see if it trains:\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.940104\n0.959786\n00:15\n\n\n1\n0.893943\n0.905222\n00:14\n\n\n2\n0.865591\n0.875238\n00:14\n\n\n3\n0.800177\n0.867468\n00:14\n\n\n4\n0.760255\n0.867455\n00:14\n\n\n\n\n\nfastai provides this model in fastai.collab if you pass use_nn=True in your call to collab_learner (including calling get_emb_sz for you), and it lets you easily create more layers. For instance, here weâ€™re creating two hidden layers, of size 100 and 50, respectively:\n\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.002747\n0.972392\n00:16\n\n\n1\n0.926903\n0.922348\n00:16\n\n\n2\n0.877160\n0.893401\n00:16\n\n\n3\n0.838334\n0.865040\n00:16\n\n\n4\n0.781666\n0.864936\n00:16\n\n\n\n\n\nlearn.model is an object of type EmbeddingNN. Letâ€™s take a look at fastaiâ€™s code for this class:\n\n@delegates(TabularModel)\nclass EmbeddingNN(TabularModel):\n    def __init__(self, emb_szs, layers, **kwargs):\n        super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs)\n\nWow, thatâ€™s not a lot of code! This class inherits from TabularModel, which is where it gets all its functionality from. In __init__ it calls the same method in TabularModel, passing n_cont=0 and out_sz=1; other than that, it only passes along whatever arguments it received.\n\nSidebar: kwargs and Delegates\nEmbeddingNN includes **kwargs as a parameter to __init__. In Python **kwargs in a parameter list means â€œput any additional keyword arguments into a dict called kwargs. And **kwargs in an argument list meansâ€insert all key/value pairs in the kwargs dict as named arguments hereâ€. This approach is used in many popular libraries, such as matplotlib, in which the main plot function simply has the signature plot(*args, **kwargs). The plot documentation says â€œThe kwargs are Line2D propertiesâ€ and then lists those properties.\nWeâ€™re using **kwargs in EmbeddingNN to avoid having to write all the arguments to TabularModel a second time, and keep them in sync. However, this makes our API quite difficult to work with, because now Jupyter Notebook doesnâ€™t know what parameters are available. Consequently things like tab completion of parameter names and pop-up lists of signatures wonâ€™t work.\nfastai resolves this by providing a special @delegates decorator, which automatically changes the signature of the class or function (EmbeddingNN in this case) to insert all of its keyword arguments into the signature.\n\n\nEnd sidebar\nAlthough the results of EmbeddingNN are a bit worse than the dot product approach (which shows the power of carefully constructing an architecture for a domain), it does allow us to do something very important: we can now directly incorporate other user and movie information, date and time information, or any other information that may be relevant to the recommendation. Thatâ€™s exactly what TabularModel does. In fact, weâ€™ve now seen that EmbeddingNN is just a TabularModel, with n_cont=0 and out_sz=1. So, weâ€™d better spend some time learning about TabularModel, and how to use it to get great results! Weâ€™ll do that in the next chapter."
  },
  {
    "objectID": "08_collab.html#conclusion",
    "href": "08_collab.html#conclusion",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Conclusion",
    "text": "Conclusion\nFor our first non-computer vision application, we looked at recommendation systems and saw how gradient descent can learn intrinsic factors or biases about items from a history of ratings. Those can then give us information about the data.\nWe also built our first model in PyTorch. We will do a lot more of this in the next section of the book, but first, letâ€™s finish our dive into the other general applications of deep learning, continuing with tabular data."
  },
  {
    "objectID": "08_collab.html#questionnaire",
    "href": "08_collab.html#questionnaire",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat problem does collaborative filtering solve?\nHow does it solve it?\nWhy might a collaborative filtering predictive model fail to be a very useful recommendation system?\nWhat does a crosstab representation of collaborative filtering data look like?\nWrite the code to create a crosstab representation of the MovieLens data (you might need to do some web searching!).\nWhat is a latent factor? Why is it â€œlatentâ€?\nWhat is a dot product? Calculate a dot product manually using pure Python with lists.\nWhat does pandas.DataFrame.merge do?\nWhat is an embedding matrix?\nWhat is the relationship between an embedding and a matrix of one-hot-encoded vectors?\nWhy do we need Embedding if we could use one-hot-encoded vectors for the same thing?\nWhat does an embedding contain before we start training (assuming weâ€™re not using a pretained model)?\nCreate a class (without peeking, if possible!) and use it.\nWhat does x[:,0] return?\nRewrite the DotProduct class (without peeking, if possible!) and train a model with it.\nWhat is a good loss function to use for MovieLens? Why?\nWhat would happen if we used cross-entropy loss with MovieLens? How would we need to change the model?\nWhat is the use of bias in a dot product model?\nWhat is another name for weight decay?\nWrite the equation for weight decay (without peeking!).\nWrite the equation for the gradient of weight decay. Why does it help reduce weights?\nWhy does reducing weights lead to better generalization?\nWhat does argsort do in PyTorch?\nDoes sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not?\nHow do you print the names and details of the layers in a model?\nWhat is the â€œbootstrapping problemâ€ in collaborative filtering?\nHow could you deal with the bootstrapping problem for new users? For new movies?\nHow can feedback loops impact collaborative filtering systems?\nWhen using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users?\nWhy is there an nn.Sequential in the CollabNN model?\nWhat kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering model?\n\n\nFurther Research\n\nTake a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If youâ€™re not sure, try reverting each change to see what happens. (NB: even the type of brackets used in forward has changed!)\nFind three other areas where collaborative filtering is being used, and find out what the pros and cons of this approach are in those areas.\nComplete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the bookâ€™s website and the fast.ai forum for ideas. Note that there are more columns in the full datasetâ€”see if you can use those too (the next chapter might give you ideas).\nCreate a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter."
  }
]